{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6335d739-34e9-4083-9c06-d267282b2daa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in ./anaconda3/lib/python3.11/site-packages (0.0.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4783f924-c944-40d0-8e6c-21f6e0bdb69d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 222, 'name': 'Bank Marketing', 'repository_url': 'https://archive.ics.uci.edu/dataset/222/bank+marketing', 'data_url': 'https://archive.ics.uci.edu/static/public/222/data.csv', 'abstract': 'The data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit (variable y).', 'area': 'Business', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 45211, 'num_features': 16, 'feature_types': ['Categorical', 'Integer'], 'demographics': ['Age', 'Occupation', 'Marital Status', 'Education Level'], 'target_col': ['y'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 2014, 'last_updated': 'Fri Aug 18 2023', 'dataset_doi': '10.24432/C5K306', 'creators': ['S. Moro', 'P. Rita', 'P. Cortez'], 'intro_paper': {'title': 'A data-driven approach to predict the success of bank telemarketing', 'authors': 'Sérgio Moro, P. Cortez, P. Rita', 'published_in': 'Decision Support Systems', 'year': 2014, 'url': 'https://www.semanticscholar.org/paper/cab86052882d126d43f72108c6cb41b295cc8a9e', 'doi': '10.1016/j.dss.2014.03.001'}, 'additional_info': {'summary': \"The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. \\n\\nThere are four datasets: \\n1) bank-additional-full.csv with all examples (41188) and 20 inputs, ordered by date (from May 2008 to November 2010), very close to the data analyzed in [Moro et al., 2014]\\n2) bank-additional.csv with 10% of the examples (4119), randomly selected from 1), and 20 inputs.\\n3) bank-full.csv with all examples and 17 inputs, ordered by date (older version of this dataset with less inputs). \\n4) bank.csv with 10% of the examples and 17 inputs, randomly selected from 3 (older version of this dataset with less inputs). \\nThe smallest datasets are provided to test more computationally demanding machine learning algorithms (e.g., SVM). \\n\\nThe classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y).\", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Input variables:\\n   # bank client data:\\n   1 - age (numeric)\\n   2 - job : type of job (categorical: \"admin.\",\"unknown\",\"unemployed\",\"management\",\"housemaid\",\"entrepreneur\",\"student\",\\n                                       \"blue-collar\",\"self-employed\",\"retired\",\"technician\",\"services\") \\n   3 - marital : marital status (categorical: \"married\",\"divorced\",\"single\"; note: \"divorced\" means divorced or widowed)\\n   4 - education (categorical: \"unknown\",\"secondary\",\"primary\",\"tertiary\")\\n   5 - default: has credit in default? (binary: \"yes\",\"no\")\\n   6 - balance: average yearly balance, in euros (numeric) \\n   7 - housing: has housing loan? (binary: \"yes\",\"no\")\\n   8 - loan: has personal loan? (binary: \"yes\",\"no\")\\n   # related with the last contact of the current campaign:\\n   9 - contact: contact communication type (categorical: \"unknown\",\"telephone\",\"cellular\") \\n  10 - day: last contact day of the month (numeric)\\n  11 - month: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\")\\n  12 - duration: last contact duration, in seconds (numeric)\\n   # other attributes:\\n  13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\\n  14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)\\n  15 - previous: number of contacts performed before this campaign and for this client (numeric)\\n  16 - poutcome: outcome of the previous marketing campaign (categorical: \"unknown\",\"other\",\"failure\",\"success\")\\n\\n  Output variable (desired target):\\n  17 - y - has the client subscribed a term deposit? (binary: \"yes\",\"no\")\\n', 'citation': None}}\n",
      "           name     role         type      demographic  \\\n",
      "0           age  Feature      Integer              Age   \n",
      "1           job  Feature  Categorical       Occupation   \n",
      "2       marital  Feature  Categorical   Marital Status   \n",
      "3     education  Feature  Categorical  Education Level   \n",
      "4       default  Feature       Binary             None   \n",
      "5       balance  Feature      Integer             None   \n",
      "6       housing  Feature       Binary             None   \n",
      "7          loan  Feature       Binary             None   \n",
      "8       contact  Feature  Categorical             None   \n",
      "9   day_of_week  Feature         Date             None   \n",
      "10        month  Feature         Date             None   \n",
      "11     duration  Feature      Integer             None   \n",
      "12     campaign  Feature      Integer             None   \n",
      "13        pdays  Feature      Integer             None   \n",
      "14     previous  Feature      Integer             None   \n",
      "15     poutcome  Feature  Categorical             None   \n",
      "16            y   Target       Binary             None   \n",
      "\n",
      "                                          description  units missing_values  \n",
      "0                                                None   None             no  \n",
      "1   type of job (categorical: 'admin.','blue-colla...   None             no  \n",
      "2   marital status (categorical: 'divorced','marri...   None             no  \n",
      "3   (categorical: 'basic.4y','basic.6y','basic.9y'...   None             no  \n",
      "4                              has credit in default?   None             no  \n",
      "5                              average yearly balance  euros             no  \n",
      "6                                   has housing loan?   None             no  \n",
      "7                                  has personal loan?   None             no  \n",
      "8   contact communication type (categorical: 'cell...   None            yes  \n",
      "9                        last contact day of the week   None             no  \n",
      "10  last contact month of year (categorical: 'jan'...   None             no  \n",
      "11   last contact duration, in seconds (numeric). ...   None             no  \n",
      "12  number of contacts performed during this campa...   None             no  \n",
      "13  number of days that passed by after the client...   None            yes  \n",
      "14  number of contacts performed before this campa...   None             no  \n",
      "15  outcome of the previous marketing campaign (ca...   None            yes  \n",
      "16          has the client subscribed a term deposit?   None             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "bank_marketing = fetch_ucirepo(id=222) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = bank_marketing.data.features \n",
    "y = bank_marketing.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(bank_marketing.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(bank_marketing.variables) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7eec35f7-a6b9-477b-9689-f2ef67eaacd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 222, 'name': 'Bank Marketing', 'repository_url': 'https://archive.ics.uci.edu/dataset/222/bank+marketing', 'data_url': 'https://archive.ics.uci.edu/static/public/222/data.csv', 'abstract': 'The data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit (variable y).', 'area': 'Business', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 45211, 'num_features': 16, 'feature_types': ['Categorical', 'Integer'], 'demographics': ['Age', 'Occupation', 'Marital Status', 'Education Level'], 'target_col': ['y'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 2014, 'last_updated': 'Fri Aug 18 2023', 'dataset_doi': '10.24432/C5K306', 'creators': ['S. Moro', 'P. Rita', 'P. Cortez'], 'intro_paper': {'title': 'A data-driven approach to predict the success of bank telemarketing', 'authors': 'Sérgio Moro, P. Cortez, P. Rita', 'published_in': 'Decision Support Systems', 'year': 2014, 'url': 'https://www.semanticscholar.org/paper/cab86052882d126d43f72108c6cb41b295cc8a9e', 'doi': '10.1016/j.dss.2014.03.001'}, 'additional_info': {'summary': \"The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. \\n\\nThere are four datasets: \\n1) bank-additional-full.csv with all examples (41188) and 20 inputs, ordered by date (from May 2008 to November 2010), very close to the data analyzed in [Moro et al., 2014]\\n2) bank-additional.csv with 10% of the examples (4119), randomly selected from 1), and 20 inputs.\\n3) bank-full.csv with all examples and 17 inputs, ordered by date (older version of this dataset with less inputs). \\n4) bank.csv with 10% of the examples and 17 inputs, randomly selected from 3 (older version of this dataset with less inputs). \\nThe smallest datasets are provided to test more computationally demanding machine learning algorithms (e.g., SVM). \\n\\nThe classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y).\", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Input variables:\\n   # bank client data:\\n   1 - age (numeric)\\n   2 - job : type of job (categorical: \"admin.\",\"unknown\",\"unemployed\",\"management\",\"housemaid\",\"entrepreneur\",\"student\",\\n                                       \"blue-collar\",\"self-employed\",\"retired\",\"technician\",\"services\") \\n   3 - marital : marital status (categorical: \"married\",\"divorced\",\"single\"; note: \"divorced\" means divorced or widowed)\\n   4 - education (categorical: \"unknown\",\"secondary\",\"primary\",\"tertiary\")\\n   5 - default: has credit in default? (binary: \"yes\",\"no\")\\n   6 - balance: average yearly balance, in euros (numeric) \\n   7 - housing: has housing loan? (binary: \"yes\",\"no\")\\n   8 - loan: has personal loan? (binary: \"yes\",\"no\")\\n   # related with the last contact of the current campaign:\\n   9 - contact: contact communication type (categorical: \"unknown\",\"telephone\",\"cellular\") \\n  10 - day: last contact day of the month (numeric)\\n  11 - month: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\")\\n  12 - duration: last contact duration, in seconds (numeric)\\n   # other attributes:\\n  13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\\n  14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)\\n  15 - previous: number of contacts performed before this campaign and for this client (numeric)\\n  16 - poutcome: outcome of the previous marketing campaign (categorical: \"unknown\",\"other\",\"failure\",\"success\")\\n\\n  Output variable (desired target):\\n  17 - y - has the client subscribed a term deposit? (binary: \"yes\",\"no\")\\n', 'citation': None}}\n",
      "           name     role         type      demographic  \\\n",
      "0           age  Feature      Integer              Age   \n",
      "1           job  Feature  Categorical       Occupation   \n",
      "2       marital  Feature  Categorical   Marital Status   \n",
      "3     education  Feature  Categorical  Education Level   \n",
      "4       default  Feature       Binary             None   \n",
      "5       balance  Feature      Integer             None   \n",
      "6       housing  Feature       Binary             None   \n",
      "7          loan  Feature       Binary             None   \n",
      "8       contact  Feature  Categorical             None   \n",
      "9   day_of_week  Feature         Date             None   \n",
      "10        month  Feature         Date             None   \n",
      "11     duration  Feature      Integer             None   \n",
      "12     campaign  Feature      Integer             None   \n",
      "13        pdays  Feature      Integer             None   \n",
      "14     previous  Feature      Integer             None   \n",
      "15     poutcome  Feature  Categorical             None   \n",
      "16            y   Target       Binary             None   \n",
      "\n",
      "                                          description  units missing_values  \n",
      "0                                                None   None             no  \n",
      "1   type of job (categorical: 'admin.','blue-colla...   None             no  \n",
      "2   marital status (categorical: 'divorced','marri...   None             no  \n",
      "3   (categorical: 'basic.4y','basic.6y','basic.9y'...   None             no  \n",
      "4                              has credit in default?   None             no  \n",
      "5                              average yearly balance  euros             no  \n",
      "6                                   has housing loan?   None             no  \n",
      "7                                  has personal loan?   None             no  \n",
      "8   contact communication type (categorical: 'cell...   None            yes  \n",
      "9                        last contact day of the week   None             no  \n",
      "10  last contact month of year (categorical: 'jan'...   None             no  \n",
      "11   last contact duration, in seconds (numeric). ...   None             no  \n",
      "12  number of contacts performed during this campa...   None             no  \n",
      "13  number of days that passed by after the client...   None            yes  \n",
      "14  number of contacts performed before this campa...   None             no  \n",
      "15  outcome of the previous marketing campaign (ca...   None            yes  \n",
      "16          has the client subscribed a term deposit?   None             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# Fetch dataset\n",
    "bank_marketing = fetch_ucirepo(id=222)\n",
    "\n",
    "# Data (as pandas dataframes)\n",
    "X = bank_marketing.data.features\n",
    "y = bank_marketing.data.targets\n",
    "\n",
    "# Metadata\n",
    "print(bank_marketing.metadata)\n",
    "\n",
    "# Variable information\n",
    "print(bank_marketing.variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8eb99d4b-c373-4a99-9659-1a865c2bfffc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Data Intake Report ###\n",
      "1. Data Source: Portuguese banking institution marketing campaigns\n",
      "2. Data Collection Period: Multiple marketing campaigns conducted by the bank\n",
      "3. Data Format: Structured format provided as a CSV file\n",
      "4. Data Size: Number of records: 45211\n",
      "5. Data Quality: Data quality issues detected. Further investigation required.\n",
      "6. Data Relevance: The dataset contains pertinent information for building predictive models.\n",
      "7. Data Privacy: Measures have been taken to ensure data privacy and compliance with regulations.\n",
      "8. Data Access: Access to the dataset is restricted to authorized project team members.\n",
      "9. Data Exploration (EDA): Perform exploratory data analysis\n",
      "10. Data Preprocessing: Perform preprocessing steps\n",
      "11. Data Documentation: Create detailed documentation\n",
      "12. Data Storage: Store the dataset securely\n",
      "13. Data Usage: The dataset will be used exclusively for model development and evaluation as outlined in the project scope.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "bank_marketing = fetch_ucirepo(id=222)\n",
    "X = bank_marketing.data.features\n",
    "y = bank_marketing.data.targets\n",
    "\n",
    "# 1. Data Source\n",
    "data_source = \"Portuguese banking institution marketing campaigns\"\n",
    "\n",
    "# 2. Data Collection Period\n",
    "data_collection_period = \"Multiple marketing campaigns conducted by the bank\"\n",
    "\n",
    "# 3. Data Format\n",
    "data_format = \"Structured format provided as a CSV file\"\n",
    "\n",
    "# 4. Data Size\n",
    "data_size = f\"Number of records: {len(X)}\"\n",
    "\n",
    "# 5. Data Quality\n",
    "# Perform initial data quality assessment\n",
    "missing_values = X.isnull().sum().sum()  # Total number of missing values\n",
    "inconsistencies = 0  # Placeholder for inconsistencies assessment\n",
    "\n",
    "if missing_values == 0 and inconsistencies == 0:\n",
    "    data_quality = \"The data appears to be relatively clean.\"\n",
    "else:\n",
    "    data_quality = \"Data quality issues detected. Further investigation required.\"\n",
    "\n",
    "# 6. Data Relevance\n",
    "data_relevance = \"The dataset contains pertinent information for building predictive models.\"\n",
    "\n",
    "# 7. Data Privacy\n",
    "data_privacy = \"Measures have been taken to ensure data privacy and compliance with regulations.\"\n",
    "\n",
    "# 8. Data Access\n",
    "data_access = \"Access to the dataset is restricted to authorized project team members.\"\n",
    "\n",
    "# 13. Data Usage\n",
    "data_usage = \"The dataset will be used exclusively for model development and evaluation as outlined in the project scope.\"\n",
    "\n",
    "# Print the data intake report\n",
    "print(\"### Data Intake Report ###\")\n",
    "print(f\"1. Data Source: {data_source}\")\n",
    "print(f\"2. Data Collection Period: {data_collection_period}\")\n",
    "print(f\"3. Data Format: {data_format}\")\n",
    "print(f\"4. Data Size: {data_size}\")\n",
    "print(f\"5. Data Quality: {data_quality}\")\n",
    "print(f\"6. Data Relevance: {data_relevance}\")\n",
    "print(f\"7. Data Privacy: {data_privacy}\")\n",
    "print(f\"8. Data Access: {data_access}\")\n",
    "print(\"9. Data Exploration (EDA): Perform exploratory data analysis\")\n",
    "print(\"10. Data Preprocessing: Perform preprocessing steps\")\n",
    "print(\"11. Data Documentation: Create detailed documentation\")\n",
    "print(\"12. Data Storage: Store the dataset securely\")\n",
    "print(f\"13. Data Usage: {data_usage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70cfc17a-3fb4-4a2b-8b51-ae065420256e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset:\n",
      "   age           job  marital  education default  balance housing loan  \\\n",
      "0   58    management  married   tertiary      no     2143     yes   no   \n",
      "1   44    technician   single  secondary      no       29     yes   no   \n",
      "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
      "3   47   blue-collar  married        NaN      no     1506     yes   no   \n",
      "4   33           NaN   single        NaN      no        1      no   no   \n",
      "\n",
      "  contact  day_of_week month  duration  campaign  pdays  previous poutcome  \n",
      "0     NaN            5   may       261         1     -1         0      NaN  \n",
      "1     NaN            5   may       151         1     -1         0      NaN  \n",
      "2     NaN            5   may        76         1     -1         0      NaN  \n",
      "3     NaN            5   may        92         1     -1         0      NaN  \n",
      "4     NaN            5   may       198         1     -1         0      NaN  \n",
      "\n",
      "Number of missing values in each column:\n",
      "age                0\n",
      "job              288\n",
      "marital            0\n",
      "education       1857\n",
      "default            0\n",
      "balance            0\n",
      "housing            0\n",
      "loan               0\n",
      "contact        13020\n",
      "day_of_week        0\n",
      "month              0\n",
      "duration           0\n",
      "campaign           0\n",
      "pdays              0\n",
      "previous           0\n",
      "poutcome       36959\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "bank_marketing = fetch_ucirepo(id=222)\n",
    "X = bank_marketing.data.features\n",
    "y = bank_marketing.data.targets\n",
    "\n",
    "# Data Understanding\n",
    "# Display the first few rows of the dataset\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(X.head())\n",
    "\n",
    "# Data Type and Problems\n",
    "# Check for missing values\n",
    "print(\"\\nNumber of missing values in each column:\")\n",
    "print(X.isnull().sum())\n",
    "\n",
    "# Check for outliers or skewed distributions (you may need to visualize)\n",
    "# Implement approaches to handle missing values, outliers, and skewed distributions\n",
    "\n",
    "# GitHub Repo Link\n",
    "github_repo_link = \"https://github.com/Panch2/Bank-Marketing-Campaign-.git\"\n",
    "print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7a5aefd-9cc6-471d-bd1e-d64de1e8cf02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age           job  education  balance  day_of_week  duration  campaign  \\\n",
      "0   58    management   tertiary     2143            5       261         1   \n",
      "1   44    technician  secondary       29            5       151         1   \n",
      "2   33  entrepreneur  secondary        2            5        76         1   \n",
      "3   47   blue-collar    unknown     1506            5        92         1   \n",
      "4   33   blue-collar    unknown        1            5       198         1   \n",
      "\n",
      "   pdays  previous  marital_married  marital_single  default_no  housing_no  \\\n",
      "0     -1         0             True           False        True       False   \n",
      "1     -1         0            False            True        True       False   \n",
      "2     -1         0             True           False        True       False   \n",
      "3     -1         0             True           False        True       False   \n",
      "4     -1         0            False            True        True        True   \n",
      "\n",
      "   housing_yes  loan_no  loan_yes  month_may  \n",
      "0         True     True     False       True  \n",
      "1         True     True     False       True  \n",
      "2         True    False      True       True  \n",
      "3         True     True     False       True  \n",
      "4        False     True     False       True  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = {\n",
    "    'age': [58, 44, 33, 47, 33],\n",
    "    'job': ['management', 'technician', 'entrepreneur', 'blue-collar', None],\n",
    "    'marital': ['married', 'single', 'married', 'married', 'single'],\n",
    "    'education': ['tertiary', 'secondary', 'secondary', None, None],\n",
    "    'default': ['no', 'no', 'no', 'no', 'no'],\n",
    "    'balance': [2143, 29, 2, 1506, 1],\n",
    "    'housing': ['yes', 'yes', 'yes', 'yes', 'no'],\n",
    "    'loan': ['no', 'no', 'yes', 'no', 'no'],\n",
    "    'contact': [None, None, None, None, None],\n",
    "    'day_of_week': [5, 5, 5, 5, 5],\n",
    "    'month': ['may', 'may', 'may', 'may', 'may'],\n",
    "    'duration': [261, 151, 76, 92, 198],\n",
    "    'campaign': [1, 1, 1, 1, 1],\n",
    "    'pdays': [-1, -1, -1, -1, -1],\n",
    "    'previous': [0, 0, 0, 0, 0],\n",
    "    'poutcome': [None, None, None, None, None]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Handling Missing Values\n",
    "df['job'] = df['job'].fillna(df['job'].mode()[0])\n",
    "df['education'] = df['education'].fillna('unknown')\n",
    "\n",
    "# Drop columns with a large number of missing values\n",
    "df.drop(columns=['contact', 'poutcome'], inplace=True)\n",
    "\n",
    "# Data Transformation\n",
    "categorical_cols = ['marital', 'default', 'housing', 'loan', 'month']\n",
    "df = pd.get_dummies(df, columns=categorical_cols)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9fa6442b-95f2-423f-bafe-8efdcf980a1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "# Handling Missing Values\n",
    "\n",
    "if not df.empty:\n",
    "    # Technique 1: Impute with Mean/Median/Mode\n",
    "    imputer = SimpleImputer(strategy='mean')  # Use 'median' or 'most_frequent' for other strategies\n",
    "    num_cols = df.select_dtypes(include='number').columns\n",
    "    df[num_cols] = imputer.fit_transform(df[num_cols])\n",
    "\n",
    "    # Technique 2: Model-based Imputation\n",
    "    # Assuming 'job' column has missing values and you want to impute them\n",
    "    # First, encode categorical variables\n",
    "    df_encoded = pd.get_dummies(df, drop_first=True)\n",
    "    # Split into train and test sets\n",
    "    train = df_encoded.dropna()\n",
    "    test = df_encoded[df_encoded.isna().any(axis=1)].dropna(axis=1)\n",
    "    # Train a model if the 'job' column still exists\n",
    "    if 'job' in train.columns:\n",
    "        model = KNeighborsRegressor()\n",
    "        model.fit(train.drop('job', axis=1), train['job'])\n",
    "        # Predict missing values\n",
    "        missing_job = model.predict(test.drop('job', axis=1))\n",
    "        # Fill missing values\n",
    "        df.loc[df_encoded.isna().any(axis=1), 'job'] = missing_job\n",
    "\n",
    "# Handling Outliers\n",
    "# Ensure that the DataFrame contains valid data before applying outlier detection techniques\n",
    "if not df.empty:\n",
    "    # Technique 1: Z-Score\n",
    "    z_scores = zscore(df.select_dtypes(include='number'))\n",
    "    abs_z_scores = abs(z_scores)\n",
    "    filtered_entries = (abs_z_scores < 3).all(axis=1)\n",
    "    df = df[filtered_entries]\n",
    "\n",
    "    # Technique 2: Interquartile Range (IQR)\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a4aa1be4-c018-4a59-af01-3b549394f25c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "Empty DataFrame\n",
      "Columns: [age, job, education, balance, day_of_week, duration, campaign, pdays, previous, marital_married, marital_single, default_no, housing_no, housing_yes, loan_no, loan_yes, month_may]\n",
      "Index: []\n",
      "\n",
      "After cleaning:\n",
      "Empty DataFrame\n",
      "Columns: [age, job, education, balance, day_of_week, duration, campaign, pdays, previous, marital_married, marital_single, default_no, housing_no, housing_yes, loan_no, loan_yes, month_may]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Before cleaning\n",
    "print(\"Before cleaning:\")\n",
    "print(df.head())\n",
    "\n",
    "# After cleaning\n",
    "print(\"\\nAfter cleaning:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45e80fb8-0812-4f43-bdbc-646c03715197",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/franciscolopez/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad862265-8ae1-4247-83a8-e6260428666e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/franciscolopez/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d1c4341-d0d6-4e95-b294-a8a36185f366",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Feature Names: ['age' 'balance' 'campaign' 'contact' 'dayofweek' 'default' 'duration'\n",
      " 'education' 'housing' 'job' 'loan' 'marital' 'month' 'pdays' 'poutcome'\n",
      " 'previous']\n",
      "TF-IDF Feature Names: ['age' 'balance' 'campaign' 'contact' 'dayofweek' 'default' 'duration'\n",
      " 'education' 'housing' 'job' 'loan' 'marital' 'month' 'pdays' 'poutcome'\n",
      " 'previous']\n"
     ]
    }
   ],
   "source": [
    "# Initialize CountVectorizer and TfidfVectorizer\n",
    "bow_vectorizer = CountVectorizer()\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the text data\n",
    "bow_matrix = bow_vectorizer.fit_transform(preprocessed_text)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_text)\n",
    "\n",
    "# Print the feature names\n",
    "print(\"BoW Feature Names:\", bow_vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Feature Names:\", tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c9a61fb6-c5af-4be5-ab72-d3a62f214ed3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Feature Names: ['age' 'balance' 'campaign' 'contact' 'dayofweek' 'default' 'duration'\n",
      " 'education' 'housing' 'job' 'loan' 'marital' 'month' 'pdays' 'poutcome'\n",
      " 'previous']\n",
      "TF-IDF Feature Names: ['age' 'balance' 'campaign' 'contact' 'dayofweek' 'default' 'duration'\n",
      " 'education' 'housing' 'job' 'loan' 'marital' 'month' 'pdays' 'poutcome'\n",
      " 'previous']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Load the dataset\n",
    "bank_marketing = fetch_ucirepo(id=222)\n",
    "X = bank_marketing.data.features\n",
    "\n",
    "# Function to clean text data using regex\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters, punctuation, and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning to each document in the dataset\n",
    "cleaned_text = [clean_text(text) for text in X]\n",
    "\n",
    "# Function to tokenize text and remove stopwords\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization and stopword removal\n",
    "tokenized_text = [tokenize_and_remove_stopwords(text) for text in cleaned_text]\n",
    "\n",
    "# Initialize a WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Apply lemmatization\n",
    "lemmatized_text = [lemmatize_tokens(tokens) for tokens in tokenized_text]\n",
    "\n",
    "# Convert tokenized and lemmatized text into strings\n",
    "preprocessed_text = [' '.join(tokens) for tokens in lemmatized_text]\n",
    "\n",
    "# Featurization Techniques\n",
    "# Bag of Words (BoW)\n",
    "bow_vectorizer = CountVectorizer()\n",
    "bow_matrix = bow_vectorizer.fit_transform(preprocessed_text)\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_text)\n",
    "\n",
    "print(\"BoW Feature Names:\", bow_vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Feature Names:\", tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a0362dd7-61cf-4312-9c98-d18ec141f1cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (/Users/franciscolopez/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, LabelEncoder\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[1;32m     12\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional-full.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/__init__.py:52\u001b[0m\n\u001b[1;32m     48\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     53\u001b[0m         combine,\n\u001b[1;32m     54\u001b[0m         ensemble,\n\u001b[1;32m     55\u001b[0m         exceptions,\n\u001b[1;32m     56\u001b[0m         metrics,\n\u001b[1;32m     57\u001b[0m         over_sampling,\n\u001b[1;32m     58\u001b[0m         pipeline,\n\u001b[1;32m     59\u001b[0m         tensorflow,\n\u001b[1;32m     60\u001b[0m         under_sampling,\n\u001b[1;32m     61\u001b[0m         utils,\n\u001b[1;32m     62\u001b[0m     )\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/combine/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mover-sampling and under-sampling.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_enn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_tomek\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEENN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTETomek\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/combine/_smote_enn.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOverSampler\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/base.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_classification_targets\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_sampling_strategy, check_target_type\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArraysTransformer\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSamplerMixin\u001b[39;00m(BaseEstimator, metaclass\u001b[38;5;241m=\u001b[39mABCMeta):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/utils/_param_validation.py:908\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_valid_param  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m--> 908\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    909\u001b[0m     HasMethods,\n\u001b[1;32m    910\u001b[0m     Hidden,\n\u001b[1;32m    911\u001b[0m     Interval,\n\u001b[1;32m    912\u001b[0m     Options,\n\u001b[1;32m    913\u001b[0m     StrOptions,\n\u001b[1;32m    914\u001b[0m     _ArrayLikes,\n\u001b[1;32m    915\u001b[0m     _Booleans,\n\u001b[1;32m    916\u001b[0m     _Callables,\n\u001b[1;32m    917\u001b[0m     _CVObjects,\n\u001b[1;32m    918\u001b[0m     _InstancesOf,\n\u001b[1;32m    919\u001b[0m     _IterablesNotString,\n\u001b[1;32m    920\u001b[0m     _MissingValues,\n\u001b[1;32m    921\u001b[0m     _NoneConstraint,\n\u001b[1;32m    922\u001b[0m     _PandasNAConstraint,\n\u001b[1;32m    923\u001b[0m     _RandomStates,\n\u001b[1;32m    924\u001b[0m     _SparseMatrices,\n\u001b[1;32m    925\u001b[0m     _VerboseHelper,\n\u001b[1;32m    926\u001b[0m     make_constraint,\n\u001b[1;32m    927\u001b[0m     validate_params,\n\u001b[1;32m    928\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (/Users/franciscolopez/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional-full.csv'\n",
    "data = pd.read_csv(url, sep=';')\n",
    "\n",
    "# Encoding categorical variables\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "for feature in categorical_features:\n",
    "    data[feature] = LabelEncoder().fit_transform(data[feature])\n",
    "\n",
    "# Handling missing values (if any)\n",
    "data = data.dropna()\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "numerical_features = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "data[numerical_features] = scaler.fit_transform(data[numerical_features])\n",
    "\n",
    "# Handling imbalanced data\n",
    "X = data.drop('y', axis=1)\n",
    "y = data['y']\n",
    "X_resampled, y_resampled = SMOTE().fit_resample(X, y)\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression Model\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg_predictions = logreg.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, logreg_predictions))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, logreg_predictions))\n",
    "\n",
    "# Random Forest Model\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "rf_predictions = rf.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, rf_predictions))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, rf_predictions))\n",
    "\n",
    "# Gradient Boosting Model\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(X_train, y_train)\n",
    "gb_predictions = gb.predict(X_test)\n",
    "\n",
    "print(\"Gradient Boosting Classification Report:\")\n",
    "print(classification_report(y_test, gb_predictions))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, gb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8608d-0032-42a8-bfe5-4cf13a95e240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f0060e-dfda-44db-9ea5-18f002786e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c75d1c-bbb1-4c41-a8b3-55e87b365a46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
